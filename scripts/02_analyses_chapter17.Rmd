---
title: "Analyses and plots for Chapter 17 -- Discussion"
subtitle: "Constituency and convergence in the Americas"
author: "Sandra Auderset"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document: 
    fig_caption: yes
    latex_engine: xelatex
    keep_tex: yes
    toc: true
mainfont: Linux Libertine
header-includes:
  - |
    ```{=latex}
    \usepackage{fvextra}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{
      breaksymbolleft={}, 
      showspaces = false,
      showtabs = false,
      breaklines,
      commandchars=\\\{\}
    }
    ```
bibliography: analysisbib.bib
csl: generic-style-rules-for-linguistics.csl
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(tidy="styler",echo=TRUE,dev = "cairo_pdf",warning = FALSE,fig.align = 'center',out.width="\\textwidth")
#out.height="\\textheight"
```


This script provides all the code for the analyses and figures presented in the discussion chapter (17) in the order of the chapter text.


```{r preamble, include=FALSE}
# load packages
library(pacman)
p_load(extrafont,
       formatR,
       ggcorrplot,
       ggmap,
       ggrepel,
       ggpubr,
       ggsci,
       ggthemes,
       here,
       janitor,
       kableExtra,
       randomForest,
       styler, 
       tidyverse,
       viridis,
       xtable)
```
```{r stadia, include=FALSE}
# API key for maps
register_stadiamaps("3a811dbc-3a12-4a65-a604-94e474a482af", write = FALSE)
```


# Section 3
## Map of the sample languages

Create map of the sample languages. This includes all the languages which are discussed in the chapters of the volume, Chacobo, which was discussed in @tallman2021constituency, and Siksika, which could not be included in the first volume but is fully analyzed (Natalie Weber, p.c.).
The map is created using Stadia Maps (stadiamaps.com) with ggmaps @kahle2013ggmap.

```{r map prep}
# read in constituency database
domains <- read_tsv(here("domains.tsv")) %>%
  modify_if(is.character, as.factor)
# read in metadata file
metadata <- read_tsv(here("input/metadata.tsv"))
# subset for languages of volume 1 and Chacobo and Siksika
metadata_sub <- metadata %>%
  filter(Contribution=="Vol1"|Language_ID=="siks1238"|Language_ID=="chac1251")

# find min/max values of lat and long for map
summary(metadata_sub$Latitude)
summary(metadata_sub$Longitude)

# get basemap
americas <- get_stadiamap(bbox = c(left=-167, bottom=-36, right=-40, top=64), maptype = "stamen_terrain", zoom = 5)

# add points for languages
sample_map <- ggmap(americas) +
  geom_point(aes(x = Longitude, y = Latitude), data = metadata_sub, alpha = 1, size = 3) +
  geom_label_repel(aes(x = Longitude, y = Latitude, label = Short_Name), data = metadata_sub, box.padding = unit(.3, "lines"), label.padding = unit(.2, "lines"), max.overlaps = 30, size = 3) +
  theme_map()
sample_map
```

We color the points by maximum number of relative convergences per language.

```{r map convergences}
# merge with domains file
metadata_sub_conv <- domains %>%
  select(Language_ID, Relative_Convergence) %>%
  group_by(Language_ID) %>%
  slice(which.max(Relative_Convergence)) %>%
  left_join(., metadata_sub)

# update map
sample_map_relconv <- ggmap(americas) +
  geom_point(aes(x = Longitude, y = Latitude, fill = Relative_Convergence), data = metadata_sub_conv, size = 4, pch = 21) +
  scale_fill_viridis(option = "inferno", direction = -1, end = 0.9,
                     name = "Max. Relative Convergence",
                     breaks = seq(0, 0.45, by = 0.05)) +
  geom_label_repel(aes(x = Longitude, y = Latitude, label = Short_Name), data = metadata_sub, box.padding = .6, point.padding = 1, max.overlaps = 30, min.segment.length = 0.1, size = 5) +
  theme_map() +
  theme(legend.position = c(0.05,0.05),
        legend.direction = "vertical",
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12))
sample_map_relconv
```
```{r export map, include=FALSE}
# export file
 ggsave("/Users/auderset/Documents/GitHub/CCAmericas/07_figures/map_languagesCCA_conv.png", sample_map_relconv, dpi = 600, height = 20, units = "cm")
```


## Dot plot of convergences and span size

We plot the relative convergences against the relative span size per sample language. We use only verbal domains as we currently have too little data for comparing nominal domains as well.

```{r dot plot}
# exclude all domains that are not verbal; add short name of languages for plotting
domains_verbal <- domains %>%
  filter(str_detect(Planar_ID, "verbal")) %>%
  filter(!is.na(Convergence)) %>%
  left_join(., select(metadata_sub, Language_ID, Short_Name)) %>%
  arrange(Relative_Size, Relative_Convergence)

# make dot plot per language
plot_rconv_facet <- ggplot(aes(x = Relative_Size, y = Relative_Convergence, group = Language_Name), data = domains_verbal) +
  geom_point(aes(), size = 2.5) +
  geom_line(linewidth = 0.5) +
  facet_wrap(~Short_Name, nrow = 6, ncol = 3, scales = "free_x") +
  ylab("Relative convergences") +
  xlab("Relative span size") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.25)) +
  scale_y_continuous(limits = c(0, 0.5)) +
  theme_bw() +
  theme(axis.text = element_text(size = 11),
        axis.title = element_text(size = 12),
        strip.text.x = element_text(size = 11),
        panel.spacing = unit(1.2, "lines"))
plot_rconv_facet
```
```{r dot export, include=FALSE}
ggsave("/Users/auderset/Documents/GitHub/CCAmericas/07_figures/plot_convergences_relative.png", plot_rconv_facet, height = 25, width = 17, device = "png", units = "cm", dpi = 600)
```


## Density plot of span size by type

We plot densities of relative span size by abstract type of test domain. Since we are doing this across languages, we exclude abstract types that have less than 10 data points, as these are not informative.

```{r density plot}
# list all abstract types
levels(domains$Abstract_Type)

# remove types with under 10 data points
include_at <- domains_verbal %>%
  count(Abstract_Type) %>%
  arrange(desc(n)) %>%
  filter(n>5) %>%
  droplevels() %>%
  pull(Abstract_Type)
domains_verbal_sub <- domains_verbal %>%
  filter(Abstract_Type %in% include_at) %>%
  droplevels()

# make density plot
plot_density_type <- ggplot(aes(x = Relative_Size, group = Abstract_Type), data = domains_verbal_sub) +
  geom_density(aes(fill = Abstract_Type, color = Abstract_Type), alpha = 0, linewidth = 1) +
  scale_color_d3(palette = "category10", guide = "none") +
  scale_fill_d3(palette = "category10", name = "Abstract Type") +
  guides(fill = guide_legend(override.aes = list(alpha = 1, linewidth = 0))) +
  labs(x = "Relative span size", y = "Density", ) +
  scale_x_continuous(expand = c(0.005, 0), breaks = seq(0, 1, 0.1)) +
  theme_bw() +
  theme(legend.position = "bottom",
        legend.key.size = unit(1, "lines"),
        legend.text = element_text(size = 11),
        axis.text = element_text(size = 12),
        axis.title = element_text(size = 13))
plot_density_type
```
```{r density export, include=FALSE}
# export plot
ggsave("/Users/auderset/Documents/GitHub/CCAmericas/07_figures/plot_size_per_test.png", plot_density_type, height = 12, width = 20, device = "png", units = "cm", dpi = 600)
```


# Section 6
## Subsection 6.2
### Correlation plots across abstract types

To look at correlations between abstract types of tests, we recode all variables into binary numeric ones, where 0 indicates absence and 1 presence.
We then sum these binary values per abstract type of test for each language and span. We repeat the same process, but for each language and left and right edges, respectively.
We create these plots with the 'ggcorrplot' package @kassambara2023ggcorrplot.

```{r corr prep}
# recode the variables to numeric
abstract_type <- domains_verbal_sub %>%
  pivot_wider(., names_from = Abstract_Type, values_from = Abstract_Type) %>%
  mutate(across(nonpermutability:noninterruption, ~if_else(is.na(.x), 0, 1)))
# sum overlaps by language and abstract type - for the whole span
abstract_type_span <- abstract_type %>%
  group_by(Language_ID, Left_Edge, Right_Edge) %>%
  summarise(Nonpermut.=sum(nonpermutability),
            Noninterrupt.=sum(noninterruption),
            Selection.=sum(ciscategorial.selection),
            Segmental.=sum(segmental),
            Free_occur.=sum(free.occurrence),
            Recursion.=sum(recursion.based),
            Supraseg.=sum(suprasegmental),
            Deviations.=sum(deviations)) %>%
  ungroup() %>%
  select(-c(Language_ID, Left_Edge,Right_Edge))
glimpse(abstract_type_span)

# # sum overlaps by language and abstract type - for the right edge
abstract_type_right <- abstract_type %>%
  group_by(Language_ID, Right_Edge) %>%
  summarise(Nonpermut.=sum(nonpermutability),
            Noninterrupt.=sum(noninterruption),
            Selection.=sum(ciscategorial.selection),
            Segmental.=sum(segmental),
            Free_occur.=sum(free.occurrence),
            Recursion.=sum(recursion.based),
            Supraseg.=sum(suprasegmental),
            Deviations.=sum(deviations)) %>%
  ungroup() %>%
  select(-c(Language_ID,Right_Edge))
glimpse(abstract_type_right)

# sum overlaps by language and abstract type - for the left edge
abstract_type_left <- abstract_type %>%
  group_by(Language_ID, Left_Edge) %>%
  summarise(Nonpermut.=sum(nonpermutability),
            Noninterrupt.=sum(noninterruption),
            Selection.=sum(ciscategorial.selection),
            Segmental.=sum(segmental),
            Free_occur.=sum(free.occurrence),
            Recursion.=sum(recursion.based),
            Supraseg.=sum(suprasegmental),
            Deviations.=sum(deviations)) %>%
  ungroup() %>%
  select(-c(Language_ID, Left_Edge))
glimpse(abstract_type_left)
```

We then calculate Kendall's tau for each of these data sets and plot the results as correlation plots.

```{r corr plots}
# make correlation matrices
corr_span <- cor(abstract_type_span, method = "kendall")
corr_left <- cor(abstract_type_left, method = "kendall")
corr_right <- cor(abstract_type_right, method = "kendall")

# plot correlations
plot_corr_span <- ggcorrplot(corr_span,
           type = "lower",
           lab = TRUE,
           lab_size = 4)
plot_corr_span

plot_corr_left <- ggcorrplot(corr_left,
                             type = "lower",
                             lab = TRUE,
                             lab_size = 5,
                             tl.cex = 12)

plot_corr_right <- ggcorrplot(corr_right,
                             type = "lower",
                             lab = TRUE,
                             lab_size = 5,
                             tl.cex = 12)
```
```{r corr export, include=FALSE}
# arrange plots for display
plots_corr_leftright <- ggarrange(plot_corr_left, plot_corr_right,
                                  #align = "v",
                                  legend = "top",
                                  common.legend = TRUE,
                                  labels = "auto")
plots_corr_leftright
# export plots
ggsave("/Users/auderset/Documents/GitHub/CCAmericas/07_figures/plot_corr_span.png", plot_corr_span, height = 14, width = 16, device = "png", units = "cm", dpi = 600)
ggsave("/Users/auderset/Documents/GitHub/CCAmericas/07_figures/plot_corr_lr.png", plots_corr_leftright, height = 14, width = 30, device = "png", units = "cm", dpi = 600)
```


### Correlation tables for cross-language and minimal/maximal fractures

We also want to look at more fine-grained fractures. We thus create new subsets with the cross-language fractures and the minimal/maximal fractures, where available.
For ciscategorial selection, free occurrence, recursion based, segmental, and suprasegmental test domains, we use the minimal/maximal fractures.
For deviations from biuniqueness, non-permutability, and non-interruptability we use the cross-language fractures.

```{r corr fine}
# data set for comparing cross-language fractures and min-max domains
cross_l <- domains_verbal_sub %>%
  mutate(MinMax_Fracture = case_match(MinMax_Fracture,
                                      "minimal" ~ "min",
                                      "maximal" ~ "max")) %>%
  mutate(abstract_cross_l = case_when(Abstract_Type=="deviations" ~ paste(Abstract_Type, CrossL_Fracture, sep = "_"),
                                      Abstract_Type=="nonpermutability" ~ paste(Abstract_Type, CrossL_Fracture, sep = "_"),
                                      Abstract_Type=="noninterruption" ~ paste(Abstract_Type, CrossL_Fracture, sep = "_"),
                                      Abstract_Type=="ciscategorial.selection" ~ paste(Abstract_Type, MinMax_Fracture, sep = "_"),
                                      Abstract_Type=="free.occurrence" ~ paste(Abstract_Type, MinMax_Fracture, sep = "_"),
                                      Abstract_Type=="recursion.based" ~ paste(Abstract_Type, MinMax_Fracture, sep = "_"),
                                      Abstract_Type=="segmental" ~ paste(Abstract_Type, MinMax_Fracture, sep = "_"),
                                      Abstract_Type=="suprasegmental" ~ paste(Abstract_Type, MinMax_Fracture, sep = "_"))) %>%
  mutate(abstract_cross_l = str_remove_all(abstract_cross_l, "_NA"),
         abstract_cross_l = if_else(Abstract_Type=="deviations", Abstract_Type, abstract_cross_l)) %>%
  arrange(abstract_cross_l) %>%
  pivot_wider(., names_from = abstract_cross_l, values_from = abstract_cross_l) %>%
  mutate(across(ciscategorial.selection:suprasegmental_min, ~if_else(is.na(.x), 0, 1)))
```

We look at correlations between minimal and maximal domains and cross-language fractures, respectively. We do this first for the whole span and then for the right and left edge.
Since there are many test pairs and we are only interested in correlations that are at least moderate, we summarize the results in tables omitting values between -0.2 and 0.2.

```{r corr crossl}
# make data set for correlations using whole span
cross_l_span <- cross_l %>%
  group_by(Language_ID, Left_Edge, Right_Edge)%>%
  summarise(Noninterrupt_simpl = sum(noninterruption_simplex),
            Noninterrupt_compl = sum(noninterruption_complex),
            Noninterrupt_multipos = sum(noninterruption_multipositional),
            Nonpermut_rigid = sum(nonpermutability_rigid),
            Nonpermut_scopal = sum(nonpermutability_scopal),
            Selection_min = sum(ciscategorial.selection_min),
            Selection_max = sum(ciscategorial.selection_max),
            Recursion_min = sum(recursion.based_min),
            Recursion_max = sum(recursion.based_max),
            FreeOccur_min = sum(free.occurrence_min),
            FreeOccur_max = sum(free.occurrence_max),
            Deviations = sum(deviations),
            Segmental_min = sum(segmental_min),
            Segmental_max = sum(segmental_max),
            Supraseg_min = sum(suprasegmental_min),
            Supraseg_max = sum(suprasegmental_min)) %>%
  ungroup()
glimpse(cross_l_span)

# minimal domains and cross-language fractures
cross_l_min <- select(cross_l_span, -ends_with("_max"), -c(Language_ID:Right_Edge))
# maximal domains and cross-language fractures
cross_l_max <- select(cross_l_span, -ends_with("_min"), -c(Language_ID:Right_Edge))
# correlation matrices
corr_cross_l_min <- cor(cross_l_min, method = "kendall")
corr_cross_l_max <- cor(cross_l_max, method = "kendall")
```

```{r corr crossl edges, message=FALSE}
# same but with edge convergences instead of whole span
# left edge:
cross_l_left <- cross_l %>%
  group_by(Language_ID, Left_Edge) %>%
  summarise(Noninterrupt_simpl = sum(noninterruption_simplex),
            Noninterrupt_compl = sum(noninterruption_complex),
            Noninterrupt_multipos = sum(noninterruption_multipositional),
            Nonpermut_rigid = sum(nonpermutability_rigid),
            Nonpermut_scopal = sum(nonpermutability_scopal),
            Selection_min = sum(ciscategorial.selection_min),
            Selection_max = sum(ciscategorial.selection_max),
            Recursion_min = sum(recursion.based_min),
            Recursion_max = sum(recursion.based_max),
            FreeOccur_min = sum(free.occurrence_min),
            FreeOccur_max = sum(free.occurrence_max),
            Deviations = sum(deviations),
            Segmental_min = sum(segmental_min),
            Segmental_max = sum(segmental_max),
            Supraseg_min = sum(suprasegmental_min),
            Supraseg_max = sum(suprasegmental_min)) %>%
  ungroup()

# right edge:
cross_l_right <- cross_l %>%
  group_by(Language_ID, Right_Edge) %>%
  summarise(Noninterrupt_simpl = sum(noninterruption_simplex),
            Noninterrupt_compl = sum(noninterruption_complex),
            Noninterrupt_multipos = sum(noninterruption_multipositional),
            Nonpermut_rigid = sum(nonpermutability_rigid),
            Nonpermut_scopal = sum(nonpermutability_scopal),
            Selection_min = sum(ciscategorial.selection_min),
            Selection_max = sum(ciscategorial.selection_max),
            Recursion_min = sum(recursion.based_min),
            Recursion_max = sum(recursion.based_max),
            FreeOccur_min = sum(free.occurrence_min),
            FreeOccur_max = sum(free.occurrence_max),
            Deviations = sum(deviations),
            Segmental_min = sum(segmental_min),
            Segmental_max = sum(segmental_max),
            Supraseg_min = sum(suprasegmental_min),
            Supraseg_max = sum(suprasegmental_min)) %>%
  ungroup()

# minimal domains and cross-language fractures - left edges
cross_l_min_left <- select(cross_l_left, -ends_with("_max"), -c(Language_ID:Left_Edge))
# maximal domains and cross-language fractures
cross_l_max_left <- select(cross_l_left, -ends_with("_min"), -c(Language_ID:Left_Edge))

# minimal domains and cross-language fractures - right edges
cross_l_min_right <- select(cross_l_right, -ends_with("_max"), -c(Language_ID:Right_Edge))
# maximal domains and cross-language fractures
cross_l_max_right <- select(cross_l_right, -ends_with("_min"), -c(Language_ID:Right_Edge))
```

To create the correlation tables efficiently, we set up a custom function.

```{r corrtable}
# set up function to create correlation dataframes
# for span
corr_to_df1 <- function(x, y, z){
  df1 <- cor(x, method = "kendall") %>%
    as.data.frame() %>%
    rownames_to_column(., var = "Test1") %>%
    pivot_longer(-Test1, names_to = "Test2", values_to = paste0(y, ".", z))
  # full correlation df
  corr_full <- df1 %>%
    distinct(across(starts_with("M")), .keep_all = TRUE) %>%
    filter(Test1!=Test2) %>%
    kable(booktabs = TRUE, linesep = "",
        align = "llrr")
  # excluding weak correlations and highlighting cells with higher correlations; exporting table to latex
  corr_higher <- df1 %>%
    distinct(across(starts_with("M")), .keep_all = TRUE) %>%
    filter(Test1!=Test2) %>%
    mutate(across(starts_with("M"), ~round(.x, 2))) %>%
    mutate(across(starts_with("M"), ~ case_when(
    .x > 0.3 ~ paste0("\\cellcolor{red!45}", .x),
    between(.x, 0.2, 0.3) ~ paste0("\\cellcolor{red!25}", .x),
    .x < -0.3 ~ paste0("\\cellcolor{blue!45}", .x),
    between(.x, 0.2, 0.3) ~ paste0("\\cellcolor{blue!25}", .x),
    .default = paste(.x)))) %>%
    mutate(across(starts_with("Test"), ~ str_replace_all(.x, "_", "."))) %>%
    filter(if_any(starts_with("M"), ~str_detect(., "cellcolor"))) %>%
    kable(booktabs = TRUE, format = "latex", escape = FALSE, linesep = "",
        align = "llrr") %>%
    kable_styling()
  # return full df and latex table
  return(list(corr_full, corr_higher))
}

# for edges
corr_to_df2 <- function(x, y, z, a, b, c){
  df1 <- cor(x, method = "kendall") %>%
    as.data.frame() %>%
    rownames_to_column(., var = "Test1") %>%
    pivot_longer(-Test1, names_to = "Test2", values_to = paste0(y, ".", z))
  df2 <- cor(a, method = "kendall") %>%
    as.data.frame() %>%
    rownames_to_column(., var = "Test1") %>%
    pivot_longer(-Test1, names_to = "Test2", values_to = paste0(b, ".", c)) %>%
    select(-c(Test1, Test2))
  # full correlation df
  corr_full <- bind_cols(df1, df2) %>%
    distinct(across(starts_with("M")), .keep_all = TRUE) %>%
    filter(Test1!=Test2) %>%
    kable(booktabs = TRUE, linesep = "",
        align = "llrr")
  # excluding weak correlations and highlighting cells with higher correlations; exporting table to latex
  corr_higher <- bind_cols(df1, df2) %>%
    distinct(across(starts_with("M")), .keep_all = TRUE) %>%
    filter(Test1!=Test2) %>%
    mutate(across(starts_with("M"), ~round(.x, 2))) %>%
    mutate(across(starts_with("M"), ~ case_when(
    .x > 0.3 ~ paste0("\\cellcolor{red!45}", .x),
    between(.x, 0.2, 0.3) ~ paste0("\\cellcolor{red!25}", .x),
    .x < -0.3 ~ paste0("\\cellcolor{blue!45}", .x),
    between(.x, 0.2, 0.3) ~ paste0("\\cellcolor{blue!25}", .x),
    .default = paste(.x)))) %>%
    mutate(across(starts_with("Test"), ~ str_replace_all(.x, "_", "."))) %>%
    filter(if_any(starts_with("M"), ~str_detect(., "cellcolor"))) %>%
    kable(booktabs = TRUE, format = "latex", escape = FALSE, linesep = "",
        align = "llrr") %>%
    kable_styling()
  # return full df and latex table
  return(list(corr_full, corr_higher))
}
```

We apply the function to each subset and display the full correlation tables. The highlighted tables are exported for inclusion in the chapter.
```{r corrtable apply}
# cross-language + min/max - whole span
corr_cross_l_min_span <- corr_to_df1(cross_l_min, "Min", "Span")
corr_cross_l_min_span[[1]]
corr_cross_l_max_span <- corr_to_df1(cross_l_max, "Max", "Span")
corr_cross_l_max_span[[1]]

# cross-language + minimal - left/right
corr_cross_l_min <- corr_to_df2(cross_l_min_left, "Min", "Left", cross_l_min_right, "Min", "Right")
corr_cross_l_min[[1]]
# cross-language + maximal - left/right
corr_cross_l_max <- corr_to_df2(cross_l_max_left, "Max", "Left", cross_l_max_right, "Max", "Right")
corr_cross_l_max[[1]]
```

```{r export latex, include=FALSE}
# span
save_kable(corr_cross_l_min_span[[2]], file = here("scripts//02_analyses_chapter17_files/corr_table_min_span.tex"))
save_kable(corr_cross_l_max_span[[2]], file = here("scripts//02_analyses_chapter17_files/corr_table_max_span.tex"))
save_kable(corr_cross_l_min[[2]], file = here("scripts/02_analyses_chapter17_files/corr_table_min.tex"))
# max
save_kable(corr_cross_l_max[[2]], file = here("scripts/02_analyses_chapter17_files/corr_table_max.tex"))
```

## Subsection 6.3
### Density plot of relative convergences across span and edges

In order to assess if there are overall tendencies for some domains to converge than others, we explore the density distributions of span and edge convergences pooled across languages of the sample.
We do this by calculating the the convergences across the span, and on the right and left edge and displaying the densities of these distributions.
```{r dens rconv}
# calculate right and left edge convergences
domains_verbal <- domains_verbal %>%
  group_by(Language_ID, Left_Edge) %>%
  mutate(Convergence_Left = n()) %>%
  group_by(Language_ID, Right_Edge) %>%
  mutate(Convergence_Right = n()) %>%
  ungroup() %>%
  mutate(Relative_Convergence_Left = round(Convergence_Left/Tests_Total, 2), Relative_Convergence_Right = round(Convergence_Right/Tests_Total, 2))
glimpse(domains_verbal)

# data set for plotting density
domains_verbal_density <- domains_verbal %>%
  rename(Span = Relative_Convergence,
         Left = Relative_Convergence_Left,
         Right = Relative_Convergence_Right) %>%
  distinct(Language_ID, Span, Right, Left) %>%
  pivot_longer(-Language_ID, names_to = "Convergence_Type", values_to = "Relative_Convergence") %>%
  mutate(Convergence_Type = factor(Convergence_Type, levels = c("Span", "Left", "Right")))

# density plot across all languages
plot_density_all <- ggplot(data = domains_verbal_density, aes(x = Relative_Convergence, group = Convergence_Type, fill = Convergence_Type)) +
  geom_density(alpha = 0.6) +
  scale_fill_d3(name = "Type of Convergence") +
  #guides(fill = guide_legend(override.aes = list(alpha = 1, linewidth = 0))) +
  labs(x = "Relative convergence", y = "Density", ) +
  scale_x_continuous(expand = c(0.005, 0.005), breaks = seq(0, 1, 0.2)) +
  theme_bw() +
  theme(legend.position = c(0.85,0.85),
        #legend.key.size = unit(1, "lines"),
        legend.text = element_text(size = 11),
        axis.text = element_text(size = 12),
        axis.title = element_text(size = 13),
        strip.text.x = element_text(size = 11))
plot_density_all
```
```{r export dens, include=FALSE}
ggsave("/Users/auderset/Documents/GitHub/CCAmericas/07_figures/plot_density_verbal_all.png", plot_density_all, height = 13, width = 20, device = "png", units = "cm", dpi = 600)
```


### Density plots of relative span convergence per abstract type

We do the same, but separating the tests by abstract type. We exclude types with fewer than 5 data points, because these are not informative.

```{r dens at}
# facet density plot of relative convergence per abstract type
plot_dens_conv_atype <- ggplot(aes(x = Relative_Convergence, group = Abstract_Type), data = domains_verbal_sub) +
  geom_density(aes(fill = Abstract_Type)) +
  scale_fill_d3(guide = "none") +
  facet_wrap(~Abstract_Type, scales = "free_x") +
  labs(x = "Relative convergence", y = "Density", ) +
  scale_x_continuous(expand = c(0.008, 0), breaks = seq(0, 5, 0.1), limits = c(0, 0.5)) +
  scale_y_continuous(breaks = seq(0, 14, 2), limits = c(0, 14)) +
  theme_bw() +
  theme(legend.position = "bottom",
        legend.key.size = unit(1, "lines"),
        legend.text = element_text(size = 11),
        axis.text = element_text(size = 12),
        axis.title = element_text(size = 13),
        strip.text.x = element_text(size = 11),
        panel.spacing = unit(1.3, "lines"),
        strip.clip = "off")
plot_dens_conv_atype
```
```{r export dens at, include=FALSE}
ggsave("/Users/auderset/Documents/GitHub/CCAmericas/07_figures/plot_density_abstract.png", plot_dens_conv_atype, height = 17, width = 20, device = "png", units = "cm", dpi = 600)
```


### Density plots of relative span convergence per prosodic word domain

Next, we look at the distributions by type of prosodic word domain. We exclude types with fewer than 5 data points, because these are not informative.

```{r dens pw}
# exclude categories with few data points
keep_pw <- domains_verbal_sub %>%
  count(PW_Pattern) %>%
  filter(n>5) %>%
  droplevels() %>%
  pull(PW_Pattern)

domains_verbal_sub_pw <- domains_verbal_sub %>%
  filter(PW_Pattern %in% keep_pw) %>%
  droplevels()

# make plot
plot_dens_conv_pw <- ggplot(aes(x = Relative_Convergence, group = PW_Pattern), data = domains_verbal_sub_pw) +
  geom_density(aes(fill = PW_Pattern)) +
  scale_fill_d3(guide = "none") +
  facet_wrap(~PW_Pattern, scales = "free_x", ncol = 3) +
  labs(x = "Relative convergence", y = "Density", ) +
  scale_x_continuous(expand = c(0.008, 0), breaks = seq(0, 5, 0.1), limits = c(0, 0.5)) +
  scale_y_continuous(breaks = seq(0, 11, 2), limits = c(0, 11)) +
  theme_bw() +
  theme(legend.position = "bottom",
        legend.key.size = unit(1, "lines"),
        legend.text = element_text(size = 11),
        axis.text = element_text(size = 12),
        axis.title = element_text(size = 13),
        strip.text.x = element_text(size = 11),
        panel.spacing = unit(1.3, "lines"),
        strip.clip = "off")
plot_dens_conv_pw
```

```{r dens pw export, include=FALSE}
ggsave("/Users/auderset/Documents/GitHub/CCAmericas/07_figures/plot_density_pw.png", plot_dens_conv_pw, height = 20, width = 20, device = "png", units = "cm", dpi = 600)
```


### Density plots of relative span convergence across cross-language fractures

Lastly, we do the same but look at cross-linguistic fractures with more than 10 tokens each. 

```{r dens crossl}
# take subset with fractures that have more than 10 tokens; exclude NA
keep_clf <- domains_verbal %>%
  filter(!is.na(CrossL_Fracture)) %>%
  count(CrossL_Fracture) %>%
  filter(n>10) %>%
  droplevels() %>%
  pull(CrossL_Fracture)
domains_verbal_clf <- domains_verbal %>%
  filter(CrossL_Fracture %in% keep_clf) %>%
  droplevels()

# make plot
plot_dens_conv_clf <- ggplot(aes(x = Relative_Convergence, group = CrossL_Fracture), data = domains_verbal_clf) +
  geom_density(aes(fill = CrossL_Fracture)) +
  scale_fill_d3(guide = "none") +
  facet_wrap(~CrossL_Fracture, scales = "free_x", ncol = 3) +
  labs(x = "Relative convergence", y = "Density", ) +
  scale_x_continuous(expand = c(0.008, 0), breaks = seq(0, 5, 0.1), limits = c(0, 0.5)) +
  scale_y_continuous(breaks = seq(0, 11, 2), limits = c(0, 11)) +
  theme_bw() +
  theme(legend.position = "bottom",
        legend.key.size = unit(1, "lines"),
        legend.text = element_text(size = 11),
        axis.text = element_text(size = 12),
        axis.title = element_text(size = 13),
        strip.text.x = element_text(size = 11),
        panel.spacing = unit(1.3, "lines"),
        strip.clip = "off")
plot_dens_conv_clf
```

```{r export dens crossl, include=FALSE}
ggsave("/Users/auderset/Documents/GitHub/CCAmericas/07_figures/plot_density_clf.png", plot_dens_conv_clf, height = 17, width = 20, device = "png", units = "cm", dpi = 600)
```

### Random Forest analysis

```{r random forest}
# function for calculating baseline and accuracy
rf_acc <- function(x){
  cm <- as.matrix(x$confusion[ , -3])
  bl <- round(max(rowSums(cm))/sum(cm), 4)
  acc <- round(sum(diag(cm))/sum(cm), 4)
  diff <- round(acc-bl, 4)
  returnlist <- c("baseline" = bl, "accuracy" = acc, "difference"=diff)
}
# set seed for making reproducible
set.seed(200)
# subset for relevant variables, turn into factors
pw_patterns <- domains_verbal_sub %>%
  pivot_wider(., names_from = PW_Pattern, values_from = PW_Pattern) %>%
  mutate(across(morphosyntactic:`other process`, ~if_else(is.na(.x), 0, 1))) %>%
  select(Serial_Order, Language_ID, morphosyntactic:`other process`)


domains_verbal_rf <- domains_verbal_sub %>%
  select(Language_ID, Relative_Size, Convergence, Relative_Convergence, Noninterrupt.:PW_tone)

domains_vars$Language <- as.factor(domains_vars$Language)
domains_vars$Convergence <- as.factor(domains_vars$Convergence)


convergence_randomforest =randomForest(as.factor(Convergence)~.,data=domains_vars,ntree=1000, mtry=2, importance=TRUE)

varImpPlot(convergence_randomforest, main = "Diagnostics predicting convergence")
confusion_matrix=convergence_randomforest$confusion
confusion_matrix
convergence_randomforest

domains_vars_01 <- domains_vars %>%
  select(-Language, -Relative_Size, -Relative_Convergence)
convergence_randomforest_01 =randomForest(as.factor(Convergence)~.,data=domains_vars_01,ntree=1000, mtry=2, importance=TRUE)
varImpPlot(convergence_randomforest_01, main = "Predicting convergence")
confusion_matrix_01<-convergence_randomforest_01$confusion
confusion_matrix_01
rf_acc(convergence_randomforest_01)

convergence_randomforest_01

nrow(domains_vars)
188/463
```


# Section 7
## Convergence and span size in phonological domains

To assess the word bisection thesis, i.e. the idea that there are phonological and grammatical words, we have a closer look at convergence and span sizes in phonological vs. morphosyntactic domains per language.
We first create the dataset by counting convergences per language with relative span sizes.

```{r conv phon, message=FALSE}
# count convergences between those domains per language
conv_bisection <- domains %>%
  unite("Spans", Left_Edge:Right_Edge, sep = "-") %>%
  group_by(Planar_ID, Domain_Type) %>%
  mutate(Total_Tests_D = n()) %>%
  group_by(Planar_ID, Domain_Type, Spans) %>%
  mutate(DConvergence = n()) %>%
  ungroup()

# with indeterminate = morphosyntactic
conv_bisection_lumped <- domains %>%
  unite("Spans", Left_Edge:Right_Edge, sep = "-") %>%
  mutate(Domain_Type_Lumped = if_else(Domain_Type=="indeterminate", "morphosyntactic", Domain_Type)) %>%
  group_by(Planar_ID, Domain_Type) %>%
  mutate(Total_Tests_DL = n()) %>%
  group_by(Planar_ID, Domain_Type, Spans) %>%
  mutate(DLConvergence = n()) %>%
  ungroup()

# add back to df for plotting
domains_bisection <- conv_bisection %>%
  left_join(., conv_bisection_lumped) %>%
  mutate(Relative_DConvergence = round(DConvergence/Total_Tests_D, 2), Relative_DLConvergence = round(DLConvergence/Total_Tests_DL, 2))
```

We then plot the result for phonological domains.
```{r conv phon plot, message=FALSE}
# plot lumped, phonological domains
plot_points_phon <- ggplot(data = filter(domains_bisection, Domain_Type_Lumped=="phonological"), aes(x = Relative_Size, y = DLConvergence)) +
  geom_line(linewidth = 0.5) +
  geom_point(size = 3, color = "darkorange2") +
  facet_wrap(~Planar_ID, scales = "free_x", ncol = 3) +
  labs(x = "Relative span size", y = "Number of convergences") +
  scale_x_continuous(expand = c(0, 0.03), breaks = seq(0, 1, 0.25), limits = c(0, 1)) +
  scale_y_continuous(expand = c(0.1, 0.1)) +
  theme_bw() +
  theme(legend.position = "top",
        legend.key.size = unit(1, "lines"),
        legend.text = element_text(size = 11),
        axis.text = element_text(size = 11),
        axis.title = element_text(size = 13),
        strip.text.x = element_text(size = 11),
        panel.spacing = unit(1.3, "lines"),
        strip.clip = "off")
plot_points_phon
```


```{r export conv phon, include=FALSE}
ggsave("/Users/auderset/Documents/GitHub/CCAmericas/07_figures/plot_points_bisection_phon.png", plot_points_phon, height = 26, width = 19, device = "png", units = "cm", dpi = 600)
```


## Convergence and span size in morphosyntactic domains

And the same for morphosyntactic domains.

```{r conv morph}
# plot lumped, morphosyntactic domains
plot_points_ms <- ggplot(data = filter(domains_bisection, Domain_Type_Lumped=="morphosyntactic"), aes(x = Relative_Size, y = DLConvergence)) +
  geom_line(linewidth = 0.5) +
  geom_point(size = 3, color = "dodgerblue3") +
  facet_wrap(~Planar_ID, scales = "free_x", ncol = 3) +
  labs(x = "Relative span size", y = "Number of convergences") +
  scale_x_continuous(expand = c(0, 0.04), breaks = seq(0, 1, 0.25), limits = c(0, 1)) +
  scale_y_continuous(expand = c(0, 0.3)) +
  theme_bw() +
  theme(legend.position = "top",
        legend.key.size = unit(1, "lines"),
        legend.text = element_text(size = 11),
        axis.text = element_text(size = 11),
        axis.title = element_text(size = 13),
        strip.text.x = element_text(size = 11),
        panel.spacing = unit(1.3, "lines"),
        strip.clip = "off")
plot_points_ms
```

```{r export conv morph, include=FALSE}
ggsave("/Users/auderset/Documents/GitHub/CCAmericas/07_figures/plot_points_bisection_ms.png", plot_points_ms, height = 26, width = 19, device = "png", units = "cm", dpi = 600)
```


# References
